{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GridAttentionBlockND(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        gating_channels,\n",
    "        inter_channels=None,\n",
    "        dimension=3,\n",
    "        mode=\"concatenation\",\n",
    "        sub_sample_factor=(2, 2, 2),\n",
    "    ) -> None:\n",
    "        \"\"\"空间注意力：注意力门\n",
    "\n",
    "        Args:\n",
    "            in_channels (_type_): 输入特征的通道数\n",
    "            gating_channels (_type_): 门控信号 g 的通道数\n",
    "            inter_channelse (_type_, optional): 中间特征的通道数. Defaults to None.\n",
    "            dimension (int, optional): 卷积维度. Defaults to 3.\n",
    "            mode (str, optional): _description_. Defaults to 'concatenation'.\n",
    "            sub_sample_factor (tuple, optional): 卷积核大小和步长，同比缩小倍数. Defaults to (2, 2, 2).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert dimension in [2, 3]\n",
    "        assert mode in [\n",
    "            \"concatenation\",\n",
    "            \"concatenation_debug\",\n",
    "            \"concatenation_residual\",\n",
    "        ]\n",
    "\n",
    "        # downsampling rate for the input featuremap\n",
    "        if isinstance(sub_sample_factor, tuple):\n",
    "            self.sub_sample_factor = sub_sample_factor\n",
    "        elif isinstance(sub_sample_factor, list):\n",
    "            self.sub_sample_factor = tuple(sub_sample_factor)\n",
    "        else:\n",
    "            self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels (pixel dimensions)\n",
    "        self.in_channels = in_channels\n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            bn = nn.BatchNorm3d\n",
    "            self.upsample_mode = \"trilinear\"\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            self.upsample_mode = \"bilinear\"\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # output transform\n",
    "        self.W = nn.Sequential(\n",
    "            conv_nd(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.in_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            bn(self.in_channels),\n",
    "        )\n",
    "\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        # theta 需要进行上采样，将 W_x 变为 W_g\n",
    "        self.W_x = conv_nd(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.inter_channels,\n",
    "            kernel_size=self.sub_sample_kernel_size,\n",
    "            stride=self.sub_sample_factor,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "        # phi\n",
    "        self.W_g = conv_nd(\n",
    "            in_channels=self.gating_channels,\n",
    "            out_channels=self.inter_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        # inter\n",
    "        self.psi = conv_nd(\n",
    "            in_channels=self.inter_channels,\n",
    "            out_channels=1,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # Define the operation\n",
    "        if mode == \"concatenation\":\n",
    "            self.operation_function = self._concatenation\n",
    "        elif mode == \"concatenation_debug\":\n",
    "            self.operation_function = self._concatenation_debug\n",
    "        elif mode == \"concatenation_residual\":\n",
    "            self.operation_function = self._concatenation_residual\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown operation function.\")\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        \"\"\"\n",
    "        x: (b, c, t, h, w)\n",
    "        g: (b, c_g, t', h', w')\n",
    "        \"\"\"\n",
    "        output = self.operation_function(x, g)\n",
    "        return output\n",
    "\n",
    "    def _concatenation(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        theta_x = self.W_x(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        phi_g = self.W_g(g)\n",
    "        # phi_g (b, i_c, t', h', w') -> phi_g (b, i_c, t/s1, h/s2, w/s3) ps: h'=h/s1\n",
    "        phi_g = F.upsample(phi_g, size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        # ReLU激活\n",
    "        f = F.relu(theta_x + phi_g, inplace=True)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        # 进行上采样，将 h/s1 恢复到 h\n",
    "        sigm_psi_f = F.upsmaple(\n",
    "            sigm_psi_f, size=input_size[2:], model=self.upsample_mode\n",
    "        )\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "\n",
    "    def _concatenation_debug(self, x, g):\n",
    "        input_size = x.size()\n",
    "        # input_size : torch.Size([16, 16, 16, 16])\n",
    "        print(f'input_size : {input_size}')\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "        \n",
    "        # theta => (b, c, t, h, w) -> -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        theta_x = self.W_x(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "        # theta_x_size : torch.Size([16, 8, 8, 8])\n",
    "        print(f'theta_x_size : {theta_x_size}')\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        phi_g = self.W_g(g)\n",
    "        # phi_g (b, i_c, t', h', w') -> phi_g (b, i_c, t/s1, h/s2, w/s3) ps: h'=h/s1\n",
    "        phi_g = F.upsample(phi_g, size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        # phi_g : torch.Size([16, 8, 8, 8])\n",
    "        print(f'phi_g : {phi_g.size()}')\n",
    "        # ReLU激活\n",
    "        f = F.softplus(theta_x + phi_g)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        # 进行上采样，将 h/s1 恢复到 h\n",
    "        sigm_psi_f = F.upsample(\n",
    "            sigm_psi_f, size=input_size[2:], mode=self.upsample_mode\n",
    "        )\n",
    "        # sigm_psi_f : torch.Size([16, 1, 16, 16])\n",
    "        print(f'sigm_psi_f : {sigm_psi_f.size()}')\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "    \n",
    "    def _concatenation_residual(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        theta_x = self.W_x(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        phi_g = self.W_g(g)\n",
    "        # phi_g (b, i_c, t', h', w') -> phi_g (b, i_c, t/s1, h/s2, w/s3) ps: h'=h/s1\n",
    "        phi_g = F.upsample(phi_g, size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        # ReLU激活\n",
    "        f = F.relu(theta_x + phi_g, inplace=True)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        f = self.psi(f).view(batch_size, 1, -1)\n",
    "        sigm_psi_f = F.softmax(f, dim=2).view(batch_size, 1, *theta_x_size[2:])\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        # 进行上采样，将 h/s1 恢复到 h\n",
    "        sigm_psi_f = F.upsmaple(\n",
    "            sigm_psi_f, size=input_size[2:], model=self.upsample_mode\n",
    "        )\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
    "                 sub_sample_factor=(2,2)):\n",
    "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
    "                                                   inter_channels=inter_channels,\n",
    "                                                   gating_channels=gating_channels,\n",
    "                                                   dimension=2, mode=mode,\n",
    "                                                   sub_sample_factor=sub_sample_factor,\n",
    "                                                   )\n",
    "\n",
    "\n",
    "class GridAttentionBlock3D(_GridAttentionBlockND):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n",
    "                 sub_sample_factor=(2,2,2)):\n",
    "        super(GridAttentionBlock3D, self).__init__(in_channels,\n",
    "                                                   inter_channels=inter_channels,\n",
    "                                                   gating_channels=gating_channels,\n",
    "                                                   dimension=3, mode=mode,\n",
    "                                                   sub_sample_factor=sub_sample_factor,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size : torch.Size([16, 16, 16, 16])\n",
      "theta_x_size : torch.Size([16, 8, 8, 8])\n",
      "phi_g : torch.Size([16, 8, 8, 8])\n",
      "sigm_psi_f : torch.Size([16, 1, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\surroundings\\miniconda\\envs\\pytorch_cuda\\lib\\site-packages\\torch\\nn\\functional.py:3769: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(16, 16, 16, 16)\n",
    "g = torch.randn(16, 12, 8, 8)\n",
    "model = GridAttentionBlock2D(\n",
    "    in_channels=16, gating_channels=12, inter_channels=8, mode=\"concatenation_debug\"\n",
    ")\n",
    "y = model(x, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
